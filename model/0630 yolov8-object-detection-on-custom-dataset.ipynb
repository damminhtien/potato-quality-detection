{"cells":[{"cell_type":"markdown","metadata":{"id":"FyRdDYkqAKN4"},"source":["## Before you start\n","\n","Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471,"status":"ok","timestamp":1687947817521,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"Y8cDtxLIBHgQ","outputId":"716847a9-a799-4549-e3f3-55542586d45b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jun 28 10:23:37 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":742,"status":"ok","timestamp":1687948017485,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"CjpPg4mGKc1v","outputId":"2720eac8-c1ec-4321-f021-9b43697777d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["import os\n","HOME = os.getcwd()\n","print(HOME)"]},{"cell_type":"markdown","metadata":{"id":"3C3EO_2zNChu"},"source":["## Install YOLOv8\n","\n","⚠️ YOLOv8 is still under heavy development. Breaking changes are being introduced almost weekly. We strive to make our YOLOv8 notebooks work with the latest version of the library. Last tests took place on **27.01.2023** with version **YOLOv8.0.20**.\n","\n","If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n","\n","YOLOv8 can be installed in two ways - from the source and via pip. This is because it is the first iteration of YOLO to have an official package."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13492,"status":"ok","timestamp":1687948042476,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"tdSMcABDNKW-","outputId":"9681eebf-deed-4434-c680-bea6956ce86e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete ✅ (4 CPUs, 25.5 GB RAM, 25.7/166.8 GB disk)\n"]}],"source":["# Pip install method (recommended)\n","\n","!pip install ultralytics==8.0.20\n","\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VOEYrlBoP9-E","executionInfo":{"status":"ok","timestamp":1687948049284,"user_tz":-420,"elapsed":3,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"}}},"outputs":[],"source":["from ultralytics import YOLO\n","\n","from IPython.display import display, Image"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":69887,"status":"ok","timestamp":1687948135058,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"BSd93ZJzZZKt","outputId":"4bf0aad6-9be8-45af-ead5-c28da5623f75"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n","Collecting roboflow\n","  Downloading roboflow-1.1.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n","  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n","Collecting cycler==0.10.0 (from roboflow)\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Collecting idna==2.10 (from roboflow)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n","Collecting pyparsing==2.4.7 (from roboflow)\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Collecting supervision (from roboflow)\n","  Downloading supervision-0.10.0-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.16)\n","Collecting wget (from roboflow)\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0)\n","Collecting requests-toolbelt (from roboflow)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.40.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=98ef5ec58ee42ecd763f27b7456b52d186c3f065b74484b1021d1b51caaf3c25\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built wget\n","Installing collected packages: wget, python-dotenv, pyparsing, idna, cycler, certifi, supervision, requests-toolbelt, roboflow\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.1.0\n","    Uninstalling pyparsing-3.1.0:\n","      Successfully uninstalled pyparsing-3.1.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.11.0\n","    Uninstalling cycler-0.11.0:\n","      Successfully uninstalled cycler-0.11.0\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2023.5.7\n","    Uninstalling certifi-2023.5.7:\n","      Successfully uninstalled certifi-2023.5.7\n","Successfully installed certifi-2022.12.7 cycler-0.10.0 idna-2.10 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-1.0.0 roboflow-1.1.0 supervision-0.10.0 wget-3.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","cycler","idna","pyparsing"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in Potato-detection-10 to yolov8: 100% [281563509 / 281563509] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to Potato-detection-10 in yolov8:: 100%|██████████| 16082/16082 [00:01<00:00, 8482.65it/s] \n"]}],"source":["!mkdir {HOME}/datasets\n","%cd {HOME}/datasets\n","\n","!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"w5tZyl6Ev1rGXY8skKuF\")\n","project = rf.workspace(\"vegetable-quality-detection\").project(\"potato-detection-3et6q\")\n","dataset = project.version(10).download(\"yolov8\")\n"]},{"cell_type":"markdown","metadata":{"id":"YUjFBKKqXa-u"},"source":["## Custom Training"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":854633,"status":"ok","timestamp":1687952006915,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"D2YkphuiaE7_","outputId":"bf34bcb1-3a7c-432f-8b5b-1756b9648233"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Ultralytics YOLOv8.0.20 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.yaml, data=/content/datasets/Potato-detection-10/data.yaml, epochs=20, patience=6, batch=16, imgsz=640, save=True, cache=True, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=0.1, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=True, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.0005, lrf=5e-05, momentum=0.937, weight_decay=0.0010078125, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3, cfg=None, v5loader=False, save_dir=runs/detect/train2\n","2023-06-28 10:33:06.510392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-28 10:33:07.355685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      2320  ultralytics.nn.modules.Conv                  [3, 80, 3, 2]                 \n","  1                  -1  1    115520  ultralytics.nn.modules.Conv                  [80, 160, 3, 2]               \n","  2                  -1  3    436800  ultralytics.nn.modules.C2f                   [160, 160, 3, True]           \n","  3                  -1  1    461440  ultralytics.nn.modules.Conv                  [160, 320, 3, 2]              \n","  4                  -1  6   3281920  ultralytics.nn.modules.C2f                   [320, 320, 6, True]           \n","  5                  -1  1   1844480  ultralytics.nn.modules.Conv                  [320, 640, 3, 2]              \n","  6                  -1  6  13117440  ultralytics.nn.modules.C2f                   [640, 640, 6, True]           \n","  7                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n","  8                  -1  3   6969600  ultralytics.nn.modules.C2f                   [640, 640, 3, True]           \n","  9                  -1  1   1025920  ultralytics.nn.modules.SPPF                  [640, 640, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 12                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 15                  -1  3   1948800  ultralytics.nn.modules.C2f                   [960, 320, 3]                 \n"," 16                  -1  1    922240  ultralytics.nn.modules.Conv                  [320, 320, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 18                  -1  3   7174400  ultralytics.nn.modules.C2f                   [960, 640, 3]                 \n"," 19                  -1  1   3687680  ultralytics.nn.modules.Conv                  [640, 640, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n"," 21                  -1  3   7379200  ultralytics.nn.modules.C2f                   [1280, 640, 3]                \n"," 22        [15, 18, 21]  1   8722783  ultralytics.nn.modules.Detect                [5, [320, 640, 640]]          \n","Model summary: 365 layers, 68157423 parameters, 68157407 gradients, 258.1 GFLOPs\n","\n","Transferred 595/595 items from pretrained weights\n","\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0005) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0010078125), 103 bias\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/Potato-detection-10/train/labels.cache... 7189 images, 0 backgrounds, 0 corrupt: 100% 7189/7189 [00:00<?, ?it/s]\n","WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 12, len(boxes) = 26550. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n","\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (8.8GB True): 100% 7189/7189 [00:14<00:00, 482.62it/s]\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Potato-detection-10/valid/labels.cache... 576 images, 0 backgrounds, 0 corrupt: 100% 576/576 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.7GB True): 100% 576/576 [00:01<00:00, 423.95it/s]\n","Image sizes 640 train, 640 val\n","Using 4 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 20 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       1/20      15.3G     0.7998     0.8433      1.179         40        640: 100% 450/450 [09:22<00:00,  1.25s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:21<00:00,  1.17s/it]\n","                   all        576       1829      0.732      0.666      0.746      0.565\n","        Damaged potato        576        221      0.529      0.535      0.539      0.417\n","       Defected potato        576        236      0.624      0.708       0.69      0.535\n","Diseased-fungal potato        576        427      0.824      0.517      0.747      0.617\n","                Potato        576        646      0.828        0.8      0.887      0.717\n","       Sprouted potato        576        299      0.852      0.771      0.865      0.539\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       2/20      15.1G     0.7956     0.8212      1.179         60        640: 100% 450/450 [09:21<00:00,  1.25s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:19<00:00,  1.10s/it]\n","                   all        576       1829      0.653      0.705      0.743      0.563\n","        Damaged potato        576        221      0.369      0.683      0.556      0.428\n","       Defected potato        576        236      0.723       0.62      0.728      0.575\n","Diseased-fungal potato        576        427      0.646      0.637      0.735      0.599\n","                Potato        576        646      0.748      0.802       0.85      0.691\n","       Sprouted potato        576        299      0.778      0.785      0.847      0.524\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       3/20      15.1G     0.8018     0.8283      1.185         33        640: 100% 450/450 [09:17<00:00,  1.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:19<00:00,  1.10s/it]\n","                   all        576       1829      0.667       0.72      0.752      0.568\n","        Damaged potato        576        221       0.37      0.655      0.548      0.433\n","       Defected potato        576        236      0.676      0.712      0.764      0.592\n","Diseased-fungal potato        576        427      0.688      0.613      0.728       0.58\n","                Potato        576        646      0.778      0.796      0.846      0.682\n","       Sprouted potato        576        299      0.821      0.826      0.874      0.555\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       4/20      14.3G     0.8059     0.8244      1.187         73        640: 100% 450/450 [09:17<00:00,  1.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:19<00:00,  1.09s/it]\n","                   all        576       1829      0.644      0.666      0.652      0.481\n","        Damaged potato        576        221      0.513      0.611       0.62      0.475\n","       Defected potato        576        236      0.774      0.654      0.766      0.617\n","Diseased-fungal potato        576        427      0.841      0.468       0.73      0.541\n","                Potato        576        646      0.235      0.827      0.266      0.216\n","       Sprouted potato        576        299      0.859      0.769       0.88      0.557\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       5/20      15.4G     0.8112     0.8189      1.184         30        640: 100% 450/450 [09:17<00:00,  1.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:19<00:00,  1.11s/it]\n","                   all        576       1829      0.716      0.678      0.763      0.577\n","        Damaged potato        576        221      0.421      0.606      0.558      0.427\n","       Defected potato        576        236      0.623      0.733      0.763      0.592\n","Diseased-fungal potato        576        427      0.875      0.458      0.743      0.609\n","                Potato        576        646      0.836       0.74      0.871      0.714\n","       Sprouted potato        576        299      0.823      0.853      0.882       0.54\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       6/20      15.4G      0.807     0.8262      1.189         45        640: 100% 450/450 [09:16<00:00,  1.24s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 18/18 [00:19<00:00,  1.09s/it]\n","                   all        576       1829      0.707      0.665      0.756      0.571\n","        Damaged potato        576        221       0.54      0.407      0.549      0.418\n","       Defected potato        576        236      0.687      0.682      0.762      0.604\n","Diseased-fungal potato        576        427      0.799       0.52      0.743      0.608\n","                Potato        576        646      0.711      0.907      0.872      0.707\n","       Sprouted potato        576        299      0.796      0.807      0.853      0.518\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","       7/20      15.4G      0.801     0.8066      1.182        135        640:  16% 70/450 [01:28<08:01,  1.27s/it]\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/yolo\", line 8, in <module>\n","    sys.exit(entrypoint())\n","  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/cfg/__init__.py\", line 249, in entrypoint\n","    getattr(model, mode)(verbose=True, **overrides)\n","  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/model.py\", line 207, in train\n","    self.trainer.train()\n","  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/trainer.py\", line 183, in train\n","    self._do_train(int(os.getenv(\"RANK\", -1)), world_size)\n","  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/yolo/engine/trainer.py\", line 309, in _do_train\n","    self.scaler.scale(self.loss).backward()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\n","    torch.autograd.backward(\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 200, in backward\n","    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","^C\n"]}],"source":["%cd {HOME}\n","\n","!yolo task=detect mode=train model=/content/best.pt data={dataset.location}/data.yaml batch=16 lr0=0.0005 lrf=0.00005 cache=True epochs=20 imgsz=640 plots=True patience=6 cos_lr=True optimizer=Adam visualize=True dropout=0.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1687935203179,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"1MScstfHhArr","outputId":"ee5f6f86-c51d-40b6-a8e0-727d76f99d58"},"outputs":[{"name":"stdout","output_type":"stream","text":["args.yaml  events.out.tfevents.1687935177.27bb17b23f46.1784.0  weights\n"]}],"source":["!ls {HOME}/runs/detect/train/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":11,"status":"error","timestamp":1687935203179,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"_J35i8Ofhjxa","outputId":"71e3e23a-7114-4227-f01e-1892d3e74790"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-9bd96b4f985f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{HOME}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{HOME}/runs/detect/train/confusion_matrix.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         super(Image, self).__init__(data=data, url=url, filename=filename, \n\u001b[0m\u001b[1;32m   1232\u001b[0m                 metadata=metadata)\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/runs/detect/train/confusion_matrix.png'"]}],"source":["%cd {HOME}\n","Image(filename=f'{HOME}/runs/detect/train/confusion_matrix.png', width=640)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-urTWUkhRmn"},"outputs":[],"source":["%cd {HOME}\n","Image(filename=f'{HOME}/runs/detect/train/results.png', width=640)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HI4nADCCj3F5"},"outputs":[],"source":["%cd {HOME}\n","Image(filename=f'{HOME}/runs/detect/train/val_batch0_pred.jpg', width=640)"]},{"cell_type":"markdown","metadata":{"id":"6ODk1VTlevxn"},"source":["## Validate Custom Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpyuwrNlXc1P"},"outputs":[],"source":["%cd {HOME}\n","\n","!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml"]},{"cell_type":"markdown","metadata":{"id":"i4eASbcWkQBq"},"source":["## Inference with Custom Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wjc1ctZykYuf"},"outputs":[],"source":["%cd {HOME}\n","!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"]},{"cell_type":"markdown","metadata":{"id":"mEYIo95n-I0S"},"source":["**NOTE:** Let's take a look at few results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbVjEtPAkz3j"},"outputs":[],"source":["import glob\n","from IPython.display import Image, display\n","\n","for image_path in glob.glob(f'{HOME}/runs/detect/predict/*.jpg')[:3]:\n","      display(Image(filename=image_path, width=600))\n","      print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"j0tsVilOCPyq"},"source":["## Deploy model on Roboflow\n","\n","Once you have finished training your YOLOv8 model, you’ll have a set of trained weights ready for use. These weights will be in the `/runs/detect/train/weights/best.pt` folder of your project. You can upload your model weights to Roboflow Deploy to use your trained weights on our infinitely scalable infrastructure.\n","\n","The `.deploy()` function in the [Roboflow pip package](https://docs.roboflow.com/python) now supports uploading YOLOv8 weights.\n","\n","To upload model weights, add the following code to the “Inference with Custom Model” section in the aforementioned notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20605,"status":"ok","timestamp":1687941329613,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"6EhBAJ2gCPZh","outputId":"526e4cfc-2278-447b-e621-45603686feed"},"outputs":[{"name":"stdout","output_type":"stream","text":["View the status of your deployment at: https://app.roboflow.com/vegetable-quality-detection/potato-detection-3et6q/deploy/10\n","Share your model with the world at: https://universe.roboflow.com/vegetable-quality-detection/potato-detection-3et6q/model/10\n"]}],"source":["project.version(dataset.version).deploy(model_type=\"yolov8\", model_path=f\"/content/drive/MyDrive/Tranning Potato/0628/train4/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5kOhjkmcV1l"},"outputs":[],"source":["#While your deployment is processing, checkout the deployment docs to take your model to most destinations https://docs.roboflow.com/inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":1442,"status":"error","timestamp":1687941342072,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"I4bpUIibcV1l","outputId":"3b6e2410-52ce-409b-e2a6-214dd6d12d3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["running inference on B-333-_JPG.rf.d8c01b93d7cdc186572992236f912af4.jpg\n"]},{"ename":"HTTPError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-1ba6ac5d9ae3>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"running inference on \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandom_test_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_loc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandom_test_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/roboflow/models/object_detection.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, image_path, hosted, format, classes, overlap, confidence, stroke, labels)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;31m# Return a prediction group if JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://detect.roboflow.com/potato-detection-3et6q/10?api_key=w5tZyl6Ev1rGXY8skKuF&name=YOUR_IMAGE.jpg&overlap=30&confidence=40&stroke=1&labels=false&format=json"]}],"source":["#Run inference on your model on a persistant, auto-scaling, cloud API\n","\n","#load model\n","model = project.version(dataset.version).model\n","\n","#choose random test set image\n","import os, random\n","test_set_loc = dataset.location + \"/test/images/\"\n","random_test_image = random.choice(os.listdir(test_set_loc))\n","print(\"running inference on \" + random_test_image)\n","\n","pred = model.predict(test_set_loc + random_test_image, confidence=40, overlap=30).json()\n","pred"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24757,"status":"ok","timestamp":1687947855528,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"},"user_tz":-420},"id":"p3WmYpNGiXAV","outputId":"1f8729ac-27b3-40d7-cc2b-09f7d7c97707"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!cp '/content/drive/MyDrive/Tranning Potato/0628/train4/weights/best.pt' /content/"],"metadata":{"id":"2G56o998ica4","executionInfo":{"status":"ok","timestamp":1687948002861,"user_tz":-420,"elapsed":10193,"user":{"displayName":"Thắng Nguyễn","userId":"02621696590451646265"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovQgOj_xSNDg"},"source":["## 🏆 Congratulations\n","\n","### Learning Resources\n","\n","Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n","\n","- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n","- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n","- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n","- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n","\n","### Convert data formats\n","\n","Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n","\n","### Connect computer vision to your project logic\n","\n","[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1X9go8bbbowSdbEeMot0GN3kLHfzIcj0j","timestamp":1687947711796},{"file_id":"1BQpsFRmL4sfdZuUdxoue6_zXJlxQp9jE","timestamp":1687934328899},{"file_id":"1Xgpquncf4IdatxLUlzmPxy70TZx5LQoc","timestamp":1687925096099},{"file_id":"1B3MGEJWP7QBVsXB882B2s7ix5ofSwwmC","timestamp":1686573612286},{"file_id":"1jFh-F807xRrO5Q9ZOuKfvA2e8ZwVweDB","timestamp":1686483188253},{"file_id":"1Ex14qkKEK9X8fw6LFiumiAy2s7H43PcX","timestamp":1686310591376},{"file_id":"1fLGryjR-Pnf6aYPmQ5fWNl3FcTTpY41Y","timestamp":1686231048868},{"file_id":"1vp_lLFlYzxJaP3AW20TQTicv6AU02XNQ","timestamp":1686152506212},{"file_id":"1eoI0GxgintYk1d2i1CQ4jgpgGnbxDgT3","timestamp":1686149562171},{"file_id":"https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb","timestamp":1686065205383}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}